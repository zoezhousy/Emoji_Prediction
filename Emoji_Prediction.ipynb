{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32bf2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import skimage.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from skimage.feature import hog\n",
    "from sklearn.metrics import accuracy_score\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from IPython.display import display\n",
    "import urllib.request\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.feature import local_binary_pattern,hog\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3c92a5",
   "metadata": {},
   "source": [
    "# Task 1: Collect emojis, 20-30 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1364c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_categories = {\n",
    "    'smiling': ['ğŸ˜€', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜', 'ğŸ˜†'],\n",
    "    'affectionate': ['ğŸ¥°', 'ğŸ˜', 'ğŸ¤©', 'ğŸ˜˜', 'ğŸ˜š'],\n",
    "    'tongue': ['ğŸ˜‹', 'ğŸ˜›', 'ğŸ˜œ', 'ğŸ¤ª', 'ğŸ˜'],\n",
    "    'hands': ['ğŸ¤—', 'ğŸ¤­', 'ğŸ«¢', 'ğŸ«£', 'ğŸ¤«'],\n",
    "    'neutral': ['ğŸ™ƒ', 'ğŸ« ', 'ğŸ¤', 'ğŸ¤¨', 'ğŸ˜'],\n",
    "    'skeptical': ['ğŸ˜’', 'ğŸ™„', 'ğŸ˜¬', 'ğŸ˜®â€ğŸ’¨', 'ğŸ¤¥'],\n",
    "    'sleepy': ['ğŸ˜Œ', 'ğŸ˜”', 'ğŸ˜ª', 'ğŸ˜´'],\n",
    "    'unwell': ['ğŸ˜·', 'ğŸ¤’', 'ğŸ¤•'],\n",
    "    'disgusting': ['ğŸ¤®','ğŸ¤¢','ğŸ¥´'],\n",
    "    'crying': ['ğŸ˜¢','ğŸ˜­','ğŸ¥²'],\n",
    "    'scared': ['ğŸ˜±','ğŸ˜¨','ğŸ˜°'],\n",
    "    'joy': ['ğŸ˜ˆ', 'ğŸ‘¿', 'ğŸ¤¡', 'ğŸ‘»', 'ğŸ‘½'],\n",
    "    'glasses': ['ğŸ¥¸','ğŸ˜','ğŸ¤“','ğŸ§'],\n",
    "    'doctor': ['ğŸ§‘â€âš•ï¸', 'ğŸ‘¨â€âš•ï¸', 'ğŸ‘©â€âš•ï¸'],\n",
    "    'student': ['ğŸ§‘â€ğŸ“', 'ğŸ‘¨â€ğŸ“', 'ğŸ‘©â€ğŸ“'],\n",
    "    'farmer': ['ğŸ§‘â€ğŸŒ¾', 'ğŸ‘¨â€ğŸŒ¾', 'ğŸ‘©â€ğŸŒ¾'],\n",
    "    'cook':['ğŸ§‘â€ğŸ³', 'ğŸ‘¨â€ğŸ³', 'ğŸ‘©â€ğŸ³'],\n",
    "    'police':['ğŸ‘®â€â™€ï¸','ğŸ‘®â€â™‚ï¸','ğŸ‘®'],\n",
    "    'teacher':['ğŸ‘©â€ğŸ«','ğŸ§‘â€ğŸ«','ğŸ‘¨â€ğŸ«'],\n",
    "    'fruit': ['ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŠ', 'ğŸ‹'],\n",
    "    'vegetable': ['ğŸ†', 'ğŸ¥”', 'ğŸ¥•', 'ğŸŒ½', 'ğŸŒ¶ï¸'],\n",
    "    'dessert': ['ğŸ¦', 'ğŸ§', 'ğŸ¨', 'ğŸ©', 'ğŸª', 'ğŸ‚'],\n",
    "    'drink': ['ğŸ¾', 'ğŸ·', 'ğŸº', 'ğŸ»', 'ğŸ¥‚', 'ğŸ¥ƒ'],\n",
    "    'award': ['ğŸ–ï¸', 'ğŸ†', 'ğŸ…', 'ğŸ¥‡', 'ğŸ¥ˆ', 'ğŸ¥‰'],\n",
    "    'geography': ['ğŸŒ', 'ğŸŒ', 'ğŸŒ', 'ğŸ”ï¸', 'â›°ï¸', 'ğŸŒ‹'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df47b5",
   "metadata": {},
   "source": [
    "## convert emoji to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f86456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_to_image(emoji: str, size: int = 64) -> np.ndarray:\n",
    "    image = Image.new(\"L\", (64,64), (255))\n",
    "    font = ImageFont.truetype(\"C:/Users/zoezh/Desktop/COMP4423/A1/submission/NotoEmoji.ttf\", 60, encoding='unic')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    draw.textbbox(xy=[0,0], text=emoji, font=font)\n",
    "    draw.text((0, 0), emoji, fill=(0), font=font)\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf96864",
   "metadata": {},
   "source": [
    "Convert emoji into image / image list: X = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09037ffb",
   "metadata": {},
   "source": [
    "Convert into labeled index: y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fbc01",
   "metadata": {},
   "source": [
    "Convert into label list: keys = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84153406",
   "metadata": {},
   "source": [
    "emoji list: emojis = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c47b80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # emoji_img_list\n",
    "y = [] # label number index\n",
    "keys = [] # label list\n",
    "emojis = []\n",
    "# len = 0 \n",
    "# values = {}\n",
    "\n",
    "for key, value in emoji_categories.items():\n",
    "    keys.append(key)\n",
    "    # values[key] = value\n",
    "    for i in value:\n",
    "        # print(i)\n",
    "        emojis.append(i)\n",
    "        img = emoji_to_image(i)\n",
    "        img_array = np.array(img).flatten()\n",
    "        X.append(img_array)\n",
    "        y.append(keys.index(key)) # append label(number)\n",
    "        # len += 1\n",
    "\n",
    "# print(emojis)\n",
    "\n",
    "labels = [] # label name for each emoji\n",
    "for j in y:\n",
    "    labels.append(keys[j])\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd8f6a",
   "metadata": {},
   "source": [
    "### Convert into np.array(X and y)\n",
    "\n",
    "### split image, label_index, emoji, label into train dataset and val dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a07b42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# split image, index, emoji, label into train dataset and val dataset\n",
    "X_train, X_val, y_train, y_val, emoji_train, emoji_val, label_train, label_val= train_test_split(X, y, emojis, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c726fc3",
   "metadata": {},
   "source": [
    "# Task 2: extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b6730eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train = []\n",
    "feat_val = []\n",
    "feat_all = []\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "def get_feat(img):\n",
    "    # return hog(img)\n",
    "    return sift_feat(img)\n",
    "    # return gray_histogram(img)\n",
    "\n",
    "def calc_distance(x, y):\n",
    "    return L2_distance(x, y)\n",
    "#     return L2_distance_sift(x, y)\n",
    "\n",
    "def sift_feat(img):\n",
    "    kps, des =  sift.detectAndCompute(img, None)\n",
    "    responses = [kp.response for kp in kps]\n",
    "    order = np.argsort(responses)[::-1]\n",
    "    return np.array(des[order[:30]])\n",
    "\n",
    "def gray_histogram(img: np.array, norm: bool = True) -> np.array:\n",
    "    if img.shape[-1] == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    hist = np.array([len(img[img == i]) for i in range(256)])\n",
    "    if norm:\n",
    "        return hist / np.size(img)\n",
    "    return hist\n",
    "\n",
    "def L2_distance(x, y):\n",
    "    return ((x - y) ** 2).sum() ** 0.5\n",
    "\n",
    "def L2_distance_sift(x, y):\n",
    "    dist = ((x[:, None] - y[None, :])**2).sum(axis=-1).min(axis=-1)\n",
    "    dist.sort()\n",
    "    return dist[:15].mean()\n",
    "\n",
    "for sample in X_train:\n",
    "    feat_train = get_feat(X_train)\n",
    "\n",
    "for sample in X_val:\n",
    "    feat_val = get_feat(X_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e8d40",
   "metadata": {},
   "source": [
    "visualize feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9b4d945",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (330480,) (0,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# feat_train = np.array(feat_train)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# feat_val = np.array(feat_val)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming feat_train and feat_val are the feature vectors extracted from the images\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Combine training and validation features for visualization\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m feat_combined \u001b[38;5;241m=\u001b[39m \u001b[43mfeat_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeat_val\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Initialize t-SNE\u001b[39;00m\n\u001b[0;32m     13\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (330480,) (0,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# feat_train = np.array(feat_train)\n",
    "# feat_val = np.array(feat_val)\n",
    "# Assuming feat_train and feat_val are the feature vectors extracted from the images\n",
    "\n",
    "# Combine training and validation features for visualization\n",
    "feat_combined = feat_train + feat_val\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform the features to 2D space\n",
    "feat_tsne = tsne.fit_transform(feat_combined)\n",
    "\n",
    "# Separate back into training and validation sets\n",
    "feat_train_tsne = feat_tsne[:len(feat_train)]\n",
    "feat_val_tsne = feat_tsne[len(feat_train):]\n",
    "\n",
    "# Visualize the feature space\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(feat_train_tsne[:, 0], feat_train_tsne[:, 1], label='Training Set', color='b')\n",
    "plt.scatter(feat_val_tsne[:, 0], feat_val_tsne[:, 1], label='Validation Set', color='r')\n",
    "plt.title('t-SNE Visualization of Feature Space')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fda70",
   "metadata": {},
   "source": [
    "# Task 3: Train a classifier - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e5b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.55%\n",
      "\n",
      "--- Test result with original dataset ---\n",
      "-----------------------------------------\n",
      "ğŸŠ with label fruit is predicted as dessert\n",
      "ğŸ˜‹ with label tongue is predicted as disgusting\n",
      "ğŸ˜† with label smiling is predicted as smiling\n",
      "ğŸ¥ˆ with label award is predicted as award\n",
      "ğŸ‘©â€ğŸ« with label teacher is predicted as farmer\n",
      "ğŸ‘©â€ğŸ³ with label cook is predicted as farmer\n",
      "ğŸ˜Œ with label sleepy is predicted as sleepy\n",
      "ğŸ˜° with label scared is predicted as scared\n",
      "ğŸ¥ƒ with label drink is predicted as drink\n",
      "ğŸ˜› with label tongue is predicted as tongue\n",
      "ğŸ† with label vegetable is predicted as joy\n",
      "ğŸ‘¿ with label joy is predicted as joy\n",
      "ğŸ˜€ with label smiling is predicted as smiling\n",
      "ğŸ¥” with label vegetable is predicted as vegetable\n",
      "ğŸ«£ with label hands is predicted as affectionate\n",
      "â›°ï¸ with label geography is predicted as geography\n",
      "ğŸ§‘â€âš•ï¸ with label doctor is predicted as farmer\n",
      "ğŸ‹ with label fruit is predicted as neutral\n",
      "ğŸ‘¨â€ğŸ³ with label cook is predicted as farmer\n",
      "ğŸ¥² with label crying is predicted as crying\n",
      "ğŸ˜œ with label tongue is predicted as tongue\n",
      "ğŸ¤• with label unwell is predicted as disgusting\n"
     ]
    }
   ],
   "source": [
    "# Train with SVM\n",
    "model = SVC(kernel= 'linear')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# test with SVM (self-values)\n",
    "y_pred = model.predict(X_val) \n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "print('\\n--- Test result with original dataset ---')\n",
    "print('-----------------------------------------')\n",
    "category_list = list(emoji_categories)\n",
    "for val, pred, l in zip(emoji_val, y_pred, label_val):\n",
    "    print(val, 'with label', l,'is predicted as', category_list[pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1625b",
   "metadata": {},
   "source": [
    "# Task 4: Test the classifier using emojis that are unseen in the training and validation processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ef8b4",
   "metadata": {},
   "source": [
    "#### first read images unseen(generated by Emoji kitchen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a529a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 1.png\n",
      "Image shape: (4096,)\n",
      "Image: 2.png\n",
      "Image shape: (4096,)\n",
      "Image: 3.png\n",
      "Image shape: (4096,)\n",
      "Image: 4.png\n",
      "Image shape: (4096,)\n",
      "Image: 5.png\n",
      "Image shape: (4096,)\n",
      "Image: 6.png\n",
      "Image shape: (4096,)\n"
     ]
    }
   ],
   "source": [
    "# Specify path to folder containing the unseen images\n",
    "folder_path = 'C:/Users/zoezh/Desktop/COMP4423/A1/submission/unseen'\n",
    "\n",
    "unseen_img_list = []\n",
    "unseen_img_fname = []\n",
    "# Iterate over the files in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    # Check if the file is an image\n",
    "    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        # Read the image using OpenCV\n",
    "        image = cv2.imread(file_path)\n",
    "\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Check if the image was successfully read\n",
    "        if image is not None:\n",
    "            # Convert the image to 64 * 64 image array and then convert color into gray, then flatten\n",
    "            image = cv2.cvtColor(cv2.resize(image, (64, 64)), cv2.COLOR_BGR2GRAY).flatten()\n",
    "\n",
    "            # Display the shape of the image array\n",
    "            print(\"Image:\", file)\n",
    "            # unseen_img_fname.append(file)\n",
    "            print(\"Image shape:\", image.shape)\n",
    "            # display(image)\n",
    "            # plt.imshow(image_rgb)\n",
    "            # plt.axis('off')  # Turn off the axis labels\n",
    "            # plt.show()\n",
    "        else:\n",
    "            print(\"Failed to read the image:\", file)\n",
    "    \n",
    "    unseen_img_list.append(image)\n",
    "\n",
    "unseen_img_list = np.array(unseen_img_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288d2dc",
   "metadata": {},
   "source": [
    "#### test with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adc6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drink\n",
      "drink\n",
      "drink\n",
      "drink\n",
      "drink\n",
      "joy\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X, y)\n",
    "unseen_pred = model.predict(unseen_img_list) \n",
    "\n",
    "for pred in unseen_pred:\n",
    "    print(category_list[pred])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf92e635",
   "metadata": {},
   "source": [
    "# Task 5: Test the model using human face emotions datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b1046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 1.jpg\n",
      "Image shape: (4096,)\n",
      "Image: 2.jpg\n",
      "Image shape: (4096,)\n",
      "Image: 3.jpg\n",
      "Image shape: (4096,)\n",
      "Image: 4.jpg\n",
      "Image shape: (4096,)\n",
      "Image: 5.jpg\n",
      "Image shape: (4096,)\n"
     ]
    }
   ],
   "source": [
    "# Specify path to folder containing the unseen images\n",
    "folder_path = 'C:/Users/zoezh/Desktop/COMP4423/A1/submission/face'\n",
    "\n",
    "face_img_list = []\n",
    "face_img_fname = []\n",
    "# Iterate over the files in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    # Check if the file is an image\n",
    "    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        # Read the image using OpenCV\n",
    "        face_image = cv2.imread(file_path)\n",
    "\n",
    "        # Check if the image was successfully read\n",
    "        if image is not None:\n",
    "            # Convert the image to a NumPy array\n",
    "            image_array = np.array(image).flatten()\n",
    "\n",
    "            # Display the shape of the image array\n",
    "            print(\"Image:\", file)\n",
    "            # unseen_img_fname.append(file)\n",
    "            print(\"Image shape:\", image.shape)\n",
    "            # display(image)\n",
    "            # plt.imshow(image_rgb)\n",
    "            # plt.axis('off')  # Turn off the axis labels\n",
    "            # plt.show()\n",
    "        else:\n",
    "            print(\"Failed to read the image:\", file)\n",
    "    \n",
    "    face_img_list.append(image)\n",
    "\n",
    "face_img_list = np.array(face_img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45233e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X, y)\n",
    "face_pred = model.predict(face_img_list) \n",
    "\n",
    "for pred in face_pred:\n",
    "    print(category_list[pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
